<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>HotRing - A Hotspot Aware In-Memory Key-Value Store | W41ter’s Bistro</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="HotRing - A Hotspot Aware In-Memory Key-Value Store" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="在 Alibaba 的生产环境中，KVS 的请求里有 50%-90% 只访问了 1% 的数据。如下图： 实现 KVS 有很多可用的索引结构，其中 HASH 用得最多。目前的 HASH 算法并没有优化热点访问，也就是说读取一条热点数据，所付出的代价和读取其他数据是一样的。如下图，传统 HASH INDEX 结构的热点数据可能分布在 collision chaining 的任意位置。 理想情况下，查找一条数据的内存访问次数应该和它的冷热层度负相关。 想要达到理想的情况，需要解决两个问题： 检测并适应 hotspot shift concurrent access HotRing 论文的做法是：针对问题 1，将传统哈希表中的 collision chain 替换成 ordered-ring。如果热点发生迁移，那么直接将 bucket header 指向新的热点 item 即可。针对问题 2，采用 lock-free 设计。 ordered ring 的结构如上图所示，整个 ring 首尾相连，一旦发现热点迁移，只需要将 bucket 的 header 更新到新热点 item 即可。这样做的好处是热点迁移时无需重新给 ring 上数据排序。 查找时从 header 开始，遍历整个 ring ；同时 Ring 上的数据会按照 &lt;tag, key&gt; 的顺序插入到合适的位置。这样做的目的是： tag 主要用来避免对 key 的比较 顺序则是用于查询时判断 ring 是否结束，否则查找时可能会受到并发更新操作的影响，无法判断是否已经遍历完整个 ring。 此外，排序还有另一个好处，根据 termination 条件，平均查找次数约为传统 collision chain 实现方式的一半。 Hotspot Shift Identification 由于 hash 的 strongly uniformed distribution，可以认为热点数据也分布在各个 bucket 中，热点迁移识别的工作主要在 bucket 内部。 论文提出了两种识别方式： random movement statistical sampling 第一种方式是每隔 R 个请求，如果第 R 个请求是 hot access，则不做任何改变；如果第 R 个请求是 cold access ，那么这个请求对应的 item 会成为新的 hot item。 这种方式是一个简单的概率实现，其缺点也非常明显：参数 R 的大小显著影响热点识别效果；如果数据访问频率分布是均匀的，或者 collision ring 中有多个热点，那么 head pointer 可能会频繁在这些热点中跳变； 另一种方式则是在 item 里记录下访问次数，根据次数选择出合适的 item 来作为新的 hot item。 猜想：是否可以使用 thread-local 级别的数据采样算法，来得到更为精确的数据，同时也避免了不必要的 CAS 操作？ 如上图所示，每个 head pointer 的前 16bits 和 item pointer 其中的 14 bits 用于存储采样信息。其中 Active 表示在这条 collision ring 上开启采样，它主要是为了进行优化：为了确保采样不对正常读写造成影响，默认情况下 Active 为 false；每 R 个请求进行一次判断，如果仍然是 hot access，则认为目前的 hot item 仍然是准确的；否则才设置 Active 为 true。一旦 Active 被设置，后续请求需要同时使用 CAS 更新 Total Counter 和 Item 的 Counter。 采样完成后，最后一个访问的线程负责计算 collision ring 上每个 item 的访问频率，并调整 hot item。（先清除 Active 的标记） Write-Intensive Hotspot with RCU HotRing 上的 key 是通过 read-copy-update 操作进行的。更改一个 key 时，需要遍历整个 collision ring ，找到待更新的 key 的前项，并更改其指针到新 item 上。所以更新操作的 Counter 应该需要记录到 hot item 的前一项中，这样算法就会选择前一项作为 hot item，因更新操作所需要的访问次数也因此降低。 Concurrent read: 读操作从 head pointer 开始遍历 HotRing，直到碰到终止条件 insert: 找到合适位置，更新前一项的 Next Item Address 即可 update 和 deletion 会复杂一些。对于 update，如果 value 在 8 字节内，可以直接通过 CAS 进行 in-place 更新。否则，需要使用两阶段提交的策略来避免异常。 如果 read-copy-update 和其他更新操作同时执行，就会上图所示的异常。以 RCU Update &amp; Insert 为例，由于 update B 和 insert C 同时进行，C 负责更新 B 的 Next Item Address，而此时 B’ 更新了 A 的 Next Item Address，最终 C 丢失，无法被访问。 解决方式是在 update\delete 某个 item 时，先标记上 Occupied bit，这样其他尝试更新该 Next Item Address 的请求会失败并进行重试，所以后续对这个 Item 进行的操作就是安全的。 Head Pointer Movement head pointer 同样也会受到并发操作的影响，主要有两种情况： 热点迁移导致的 head pointer 更新 其他 update 和 deletion 操作 对于 case 1，head pointer 在迁移前，需要设置新 hot item 为 Occupied 保证这个过程中该节点不会被 update 或 delete。 对于 update head pointer 指向的 item，只需要在替换时设置上新 item 的 Occupied 即可；对于 delete head pointer 指向的 item，还需要设置 head pointer 指向的新 item 的 Occupied。 Lock free Rehash 传统的 Hash Table 使用 load factor 来出发 rehash，这个过程显然没有考虑到 hotspot 的影响。HotRing 使用 access overhead 来出发 rehash。 由于 HotRing 是有序的，rehash 时只需要从中间某个位置断开，生成两个新的 HotRing 即可。这个阶段主要分为三步： 评估 略" />
<meta property="og:description" content="在 Alibaba 的生产环境中，KVS 的请求里有 50%-90% 只访问了 1% 的数据。如下图： 实现 KVS 有很多可用的索引结构，其中 HASH 用得最多。目前的 HASH 算法并没有优化热点访问，也就是说读取一条热点数据，所付出的代价和读取其他数据是一样的。如下图，传统 HASH INDEX 结构的热点数据可能分布在 collision chaining 的任意位置。 理想情况下，查找一条数据的内存访问次数应该和它的冷热层度负相关。 想要达到理想的情况，需要解决两个问题： 检测并适应 hotspot shift concurrent access HotRing 论文的做法是：针对问题 1，将传统哈希表中的 collision chain 替换成 ordered-ring。如果热点发生迁移，那么直接将 bucket header 指向新的热点 item 即可。针对问题 2，采用 lock-free 设计。 ordered ring 的结构如上图所示，整个 ring 首尾相连，一旦发现热点迁移，只需要将 bucket 的 header 更新到新热点 item 即可。这样做的好处是热点迁移时无需重新给 ring 上数据排序。 查找时从 header 开始，遍历整个 ring ；同时 Ring 上的数据会按照 &lt;tag, key&gt; 的顺序插入到合适的位置。这样做的目的是： tag 主要用来避免对 key 的比较 顺序则是用于查询时判断 ring 是否结束，否则查找时可能会受到并发更新操作的影响，无法判断是否已经遍历完整个 ring。 此外，排序还有另一个好处，根据 termination 条件，平均查找次数约为传统 collision chain 实现方式的一半。 Hotspot Shift Identification 由于 hash 的 strongly uniformed distribution，可以认为热点数据也分布在各个 bucket 中，热点迁移识别的工作主要在 bucket 内部。 论文提出了两种识别方式： random movement statistical sampling 第一种方式是每隔 R 个请求，如果第 R 个请求是 hot access，则不做任何改变；如果第 R 个请求是 cold access ，那么这个请求对应的 item 会成为新的 hot item。 这种方式是一个简单的概率实现，其缺点也非常明显：参数 R 的大小显著影响热点识别效果；如果数据访问频率分布是均匀的，或者 collision ring 中有多个热点，那么 head pointer 可能会频繁在这些热点中跳变； 另一种方式则是在 item 里记录下访问次数，根据次数选择出合适的 item 来作为新的 hot item。 猜想：是否可以使用 thread-local 级别的数据采样算法，来得到更为精确的数据，同时也避免了不必要的 CAS 操作？ 如上图所示，每个 head pointer 的前 16bits 和 item pointer 其中的 14 bits 用于存储采样信息。其中 Active 表示在这条 collision ring 上开启采样，它主要是为了进行优化：为了确保采样不对正常读写造成影响，默认情况下 Active 为 false；每 R 个请求进行一次判断，如果仍然是 hot access，则认为目前的 hot item 仍然是准确的；否则才设置 Active 为 true。一旦 Active 被设置，后续请求需要同时使用 CAS 更新 Total Counter 和 Item 的 Counter。 采样完成后，最后一个访问的线程负责计算 collision ring 上每个 item 的访问频率，并调整 hot item。（先清除 Active 的标记） Write-Intensive Hotspot with RCU HotRing 上的 key 是通过 read-copy-update 操作进行的。更改一个 key 时，需要遍历整个 collision ring ，找到待更新的 key 的前项，并更改其指针到新 item 上。所以更新操作的 Counter 应该需要记录到 hot item 的前一项中，这样算法就会选择前一项作为 hot item，因更新操作所需要的访问次数也因此降低。 Concurrent read: 读操作从 head pointer 开始遍历 HotRing，直到碰到终止条件 insert: 找到合适位置，更新前一项的 Next Item Address 即可 update 和 deletion 会复杂一些。对于 update，如果 value 在 8 字节内，可以直接通过 CAS 进行 in-place 更新。否则，需要使用两阶段提交的策略来避免异常。 如果 read-copy-update 和其他更新操作同时执行，就会上图所示的异常。以 RCU Update &amp; Insert 为例，由于 update B 和 insert C 同时进行，C 负责更新 B 的 Next Item Address，而此时 B’ 更新了 A 的 Next Item Address，最终 C 丢失，无法被访问。 解决方式是在 update\delete 某个 item 时，先标记上 Occupied bit，这样其他尝试更新该 Next Item Address 的请求会失败并进行重试，所以后续对这个 Item 进行的操作就是安全的。 Head Pointer Movement head pointer 同样也会受到并发操作的影响，主要有两种情况： 热点迁移导致的 head pointer 更新 其他 update 和 deletion 操作 对于 case 1，head pointer 在迁移前，需要设置新 hot item 为 Occupied 保证这个过程中该节点不会被 update 或 delete。 对于 update head pointer 指向的 item，只需要在替换时设置上新 item 的 Occupied 即可；对于 delete head pointer 指向的 item，还需要设置 head pointer 指向的新 item 的 Occupied。 Lock free Rehash 传统的 Hash Table 使用 load factor 来出发 rehash，这个过程显然没有考虑到 hotspot 的影响。HotRing 使用 access overhead 来出发 rehash。 由于 HotRing 是有序的，rehash 时只需要从中间某个位置断开，生成两个新的 HotRing 即可。这个阶段主要分为三步： 评估 略" />
<link rel="canonical" href="/papers/2022/03/29/HotRing-A-Hotspot-Aware-In-Memory-Key-Value-Store.html" />
<meta property="og:url" content="/papers/2022/03/29/HotRing-A-Hotspot-Aware-In-Memory-Key-Value-Store.html" />
<meta property="og:site_name" content="W41ter’s Bistro" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-29T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="HotRing - A Hotspot Aware In-Memory Key-Value Store" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-29T00:00:00+08:00","datePublished":"2022-03-29T00:00:00+08:00","description":"在 Alibaba 的生产环境中，KVS 的请求里有 50%-90% 只访问了 1% 的数据。如下图： 实现 KVS 有很多可用的索引结构，其中 HASH 用得最多。目前的 HASH 算法并没有优化热点访问，也就是说读取一条热点数据，所付出的代价和读取其他数据是一样的。如下图，传统 HASH INDEX 结构的热点数据可能分布在 collision chaining 的任意位置。 理想情况下，查找一条数据的内存访问次数应该和它的冷热层度负相关。 想要达到理想的情况，需要解决两个问题： 检测并适应 hotspot shift concurrent access HotRing 论文的做法是：针对问题 1，将传统哈希表中的 collision chain 替换成 ordered-ring。如果热点发生迁移，那么直接将 bucket header 指向新的热点 item 即可。针对问题 2，采用 lock-free 设计。 ordered ring 的结构如上图所示，整个 ring 首尾相连，一旦发现热点迁移，只需要将 bucket 的 header 更新到新热点 item 即可。这样做的好处是热点迁移时无需重新给 ring 上数据排序。 查找时从 header 开始，遍历整个 ring ；同时 Ring 上的数据会按照 &lt;tag, key&gt; 的顺序插入到合适的位置。这样做的目的是： tag 主要用来避免对 key 的比较 顺序则是用于查询时判断 ring 是否结束，否则查找时可能会受到并发更新操作的影响，无法判断是否已经遍历完整个 ring。 此外，排序还有另一个好处，根据 termination 条件，平均查找次数约为传统 collision chain 实现方式的一半。 Hotspot Shift Identification 由于 hash 的 strongly uniformed distribution，可以认为热点数据也分布在各个 bucket 中，热点迁移识别的工作主要在 bucket 内部。 论文提出了两种识别方式： random movement statistical sampling 第一种方式是每隔 R 个请求，如果第 R 个请求是 hot access，则不做任何改变；如果第 R 个请求是 cold access ，那么这个请求对应的 item 会成为新的 hot item。 这种方式是一个简单的概率实现，其缺点也非常明显：参数 R 的大小显著影响热点识别效果；如果数据访问频率分布是均匀的，或者 collision ring 中有多个热点，那么 head pointer 可能会频繁在这些热点中跳变； 另一种方式则是在 item 里记录下访问次数，根据次数选择出合适的 item 来作为新的 hot item。 猜想：是否可以使用 thread-local 级别的数据采样算法，来得到更为精确的数据，同时也避免了不必要的 CAS 操作？ 如上图所示，每个 head pointer 的前 16bits 和 item pointer 其中的 14 bits 用于存储采样信息。其中 Active 表示在这条 collision ring 上开启采样，它主要是为了进行优化：为了确保采样不对正常读写造成影响，默认情况下 Active 为 false；每 R 个请求进行一次判断，如果仍然是 hot access，则认为目前的 hot item 仍然是准确的；否则才设置 Active 为 true。一旦 Active 被设置，后续请求需要同时使用 CAS 更新 Total Counter 和 Item 的 Counter。 采样完成后，最后一个访问的线程负责计算 collision ring 上每个 item 的访问频率，并调整 hot item。（先清除 Active 的标记） Write-Intensive Hotspot with RCU HotRing 上的 key 是通过 read-copy-update 操作进行的。更改一个 key 时，需要遍历整个 collision ring ，找到待更新的 key 的前项，并更改其指针到新 item 上。所以更新操作的 Counter 应该需要记录到 hot item 的前一项中，这样算法就会选择前一项作为 hot item，因更新操作所需要的访问次数也因此降低。 Concurrent read: 读操作从 head pointer 开始遍历 HotRing，直到碰到终止条件 insert: 找到合适位置，更新前一项的 Next Item Address 即可 update 和 deletion 会复杂一些。对于 update，如果 value 在 8 字节内，可以直接通过 CAS 进行 in-place 更新。否则，需要使用两阶段提交的策略来避免异常。 如果 read-copy-update 和其他更新操作同时执行，就会上图所示的异常。以 RCU Update &amp; Insert 为例，由于 update B 和 insert C 同时进行，C 负责更新 B 的 Next Item Address，而此时 B’ 更新了 A 的 Next Item Address，最终 C 丢失，无法被访问。 解决方式是在 update\\delete 某个 item 时，先标记上 Occupied bit，这样其他尝试更新该 Next Item Address 的请求会失败并进行重试，所以后续对这个 Item 进行的操作就是安全的。 Head Pointer Movement head pointer 同样也会受到并发操作的影响，主要有两种情况： 热点迁移导致的 head pointer 更新 其他 update 和 deletion 操作 对于 case 1，head pointer 在迁移前，需要设置新 hot item 为 Occupied 保证这个过程中该节点不会被 update 或 delete。 对于 update head pointer 指向的 item，只需要在替换时设置上新 item 的 Occupied 即可；对于 delete head pointer 指向的 item，还需要设置 head pointer 指向的新 item 的 Occupied。 Lock free Rehash 传统的 Hash Table 使用 load factor 来出发 rehash，这个过程显然没有考虑到 hotspot 的影响。HotRing 使用 access overhead 来出发 rehash。 由于 HotRing 是有序的，rehash 时只需要从中间某个位置断开，生成两个新的 HotRing 即可。这个阶段主要分为三步： 评估 略","headline":"HotRing - A Hotspot Aware In-Memory Key-Value Store","mainEntityOfPage":{"@type":"WebPage","@id":"/papers/2022/03/29/HotRing-A-Hotspot-Aware-In-Memory-Key-Value-Store.html"},"url":"/papers/2022/03/29/HotRing-A-Hotspot-Aware-In-Memory-Key-Value-Store.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="W41ter's Bistro" /><!-- for mathjax support -->
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">W41ter&#39;s Bistro</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">HotRing - A Hotspot Aware In-Memory Key-Value Store</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-03-29T00:00:00+08:00" itemprop="datePublished">Mar 29, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>在 Alibaba 的生产环境中，KVS 的请求里有 50%-90% 只访问了 1% 的数据。如下图：</p>

<p><img src="/uploads/images/2022/hotring-1.png" alt="Figure 1: Access ratio of different keys." /></p>

<p>实现 KVS 有很多可用的索引结构，其中 HASH 用得最多。目前的 HASH 算法并没有优化热点访问，也就是说读取一条热点数据，所付出的代价和读取其他数据是一样的。如下图，传统 HASH INDEX 结构的热点数据可能分布在 collision chaining 的任意位置。</p>

<p><img src="/uploads/images/2022/hotring-2.png" alt="Figure 2: The conventional hash index structure" /></p>

<p>理想情况下，查找一条数据的内存访问次数应该和它的冷热层度负相关。</p>

<p><img src="/uploads/images/2022/hotring-3.png" alt="Figure 3: Expected memory accesses for an index lookup" /></p>

<p>想要达到理想的情况，需要解决两个问题：</p>
<ol>
  <li>检测并适应 hotspot shift</li>
  <li>concurrent access</li>
</ol>

<h2 id="hotring">HotRing</h2>

<p>论文的做法是：针对问题 1，将传统哈希表中的 collision chain 替换成 ordered-ring。如果热点发生迁移，那么直接将 bucket header 指向新的热点 item 即可。针对问题 2，采用 lock-free 设计。</p>

<p><img src="/uploads/images/2022/hotring-4.png" alt="Figure 4: The index structure of HotRing" /></p>

<p>ordered ring 的结构如上图所示，整个 ring 首尾相连，一旦发现热点迁移，只需要将 bucket 的 header 更新到新热点 item 即可。这样做的好处是热点迁移时无需重新给 ring 上数据排序。</p>

<p>查找时从 header 开始，遍历整个 ring ；同时 Ring 上的数据会按照 <code class="language-plaintext highlighter-rouge">&lt;tag, key&gt;</code> 的顺序插入到合适的位置。这样做的目的是：</p>
<ul>
  <li>tag 主要用来避免对 key 的比较</li>
  <li>顺序则是用于查询时判断 ring 是否结束，否则查找时可能会受到并发更新操作的影响，无法判断是否已经遍历完整个 ring。</li>
</ul>

<p><img src="/uploads/images/2022/hotring-5.png" alt="Figure 5: Lookup Termination" /></p>

<p>此外，排序还有另一个好处，根据 termination 条件，平均查找次数约为传统 collision chain 实现方式的一半。</p>

<h2 id="hotspot-shift-identification">Hotspot Shift Identification</h2>

<p>由于 hash 的 strongly uniformed distribution，可以认为热点数据也分布在各个 bucket 中，热点迁移识别的工作主要在 bucket 内部。</p>

<p>论文提出了两种识别方式：</p>
<ol>
  <li>random movement</li>
  <li>statistical sampling</li>
</ol>

<p>第一种方式是每隔 R 个请求，如果第 R 个请求是 hot access，则不做任何改变；如果第 R 个请求是 cold access ，那么这个请求对应的 item 会成为新的 hot item。</p>

<p>这种方式是一个简单的概率实现，其缺点也非常明显：参数 R 的大小显著影响热点识别效果；如果数据访问频率分布是均匀的，或者 collision ring 中有多个热点，那么 head pointer 可能会频繁在这些热点中跳变；</p>

<p>另一种方式则是在 item 里记录下访问次数，根据次数选择出合适的 item 来作为新的 hot item。</p>

<blockquote>
  <p>猜想：是否可以使用 thread-local 级别的数据采样算法，来得到更为精确的数据，同时也避免了不必要的 CAS 操作？</p>
</blockquote>

<p><img src="/uploads/images/2022/hotring-6.png" alt="Figure 6: Index Format" /></p>

<p>如上图所示，每个 head pointer 的前 16bits 和 item pointer 其中的 14 bits 用于存储采样信息。其中 Active 表示在这条 collision ring 上开启采样，它主要是为了进行优化：为了确保采样不对正常读写造成影响，默认情况下 Active 为 false；每 R 个请求进行一次判断，如果仍然是 hot access，则认为目前的 hot item 仍然是准确的；否则才设置 Active 为 true。一旦 Active 被设置，后续请求需要同时使用 CAS 更新 Total Counter 和 Item 的 Counter。</p>

<p>采样完成后，最后一个访问的线程负责计算 collision ring 上每个 item 的访问频率，并调整 hot item。（先清除 Active 的标记）</p>

<h3 id="write-intensive-hotspot-with-rcu">Write-Intensive Hotspot with RCU</h3>

<p>HotRing 上的 key 是通过 read-copy-update 操作进行的。更改一个 key 时，需要遍历整个 collision ring ，找到待更新的 key 的前项，并更改其指针到新 item 上。所以更新操作的 Counter 应该需要记录到 hot item 的前一项中，这样算法就会选择前一项作为 hot item，因更新操作所需要的访问次数也因此降低。</p>

<p><img src="/uploads/images/2022/hotring-7.png" alt="Figure 7: Update a Hot Item A with RCU makes item F hot" /></p>

<h2 id="concurrent">Concurrent</h2>

<ul>
  <li>read: 读操作从 head pointer 开始遍历 HotRing，直到碰到终止条件</li>
  <li>insert: 找到合适位置，更新前一项的 Next Item Address 即可</li>
</ul>

<p>update 和 deletion 会复杂一些。对于 update，如果 value 在 8 字节内，可以直接通过 CAS 进行 in-place 更新。否则，需要使用两阶段提交的策略来避免异常。</p>

<p><img src="/uploads/images/2022/hotring-8.png" alt="Figure 8: Concurrent issues" /></p>

<p>如果 read-copy-update 和其他更新操作同时执行，就会上图所示的异常。以 RCU Update &amp; Insert 为例，由于 update B 和 insert C 同时进行，C 负责更新 B 的 Next Item Address，而此时 B’ 更新了 A 的 Next Item Address，最终 C 丢失，无法被访问。</p>

<p>解决方式是在 update\delete 某个 item 时，先标记上 Occupied bit，这样其他尝试更新该 Next Item Address 的请求会失败并进行重试，所以后续对这个 Item 进行的操作就是安全的。</p>

<h3 id="head-pointer-movement">Head Pointer Movement</h3>

<p>head pointer 同样也会受到并发操作的影响，主要有两种情况：</p>
<ol>
  <li>热点迁移导致的 head pointer 更新</li>
  <li>其他 update 和 deletion 操作</li>
</ol>

<p>对于 case 1，head pointer 在迁移前，需要设置新 hot item 为 Occupied 保证这个过程中该节点不会被 update 或 delete。</p>

<p>对于 update head pointer 指向的 item，只需要在替换时设置上新 item 的 Occupied 即可；对于 delete head pointer 指向的 item，还需要设置 head pointer 指向的新 item 的 Occupied。</p>

<h2 id="lock-free-rehash">Lock free Rehash</h2>

<p>传统的 Hash Table 使用 load factor 来出发 rehash，这个过程显然没有考虑到 hotspot 的影响。HotRing 使用 access overhead 来出发 rehash。</p>

<p>由于 HotRing 是有序的，rehash 时只需要从中间某个位置断开，生成两个新的 HotRing 即可。这个阶段主要分为三步：</p>

<p><img src="/uploads/images/2022/hotring-9.png" alt="Figure 9: Rehash" /></p>

<h2 id="评估">评估</h2>

<p>略</p>

  </div><a class="u-url" href="/papers/2022/03/29/HotRing-A-Hotspot-Aware-In-Memory-Key-Value-Store.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">W41ter&#39;s Bistro</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">W41ter&#39;s Bistro</li><li><a class="u-email" href="mailto:w41ter.l@gmail.com">w41ter.l@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/w41ter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">w41ter</span></a></li><li><a href="https://www.twitter.com/WalterM56697798"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">WalterM56697798</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Focus on distributed storage system, compiler.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
